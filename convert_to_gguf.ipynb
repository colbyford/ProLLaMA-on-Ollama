{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProLLaMA on Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ProLLaMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colby/Library/Python/3.8/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colby/Library/Python/3.8/lib/python/site-packages/huggingface_hub/file_download.py:1212: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 10 files: 100%|██████████| 10/10 [03:51<00:00, 23.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/colby/ProLLaMA/ollama/models/prollama'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Download the ProLLaMA model from a snapshot\n",
    "\n",
    "snapshot_download(repo_id=\"GreatCaptainNemo/ProLLaMA\",\n",
    "                  local_dir=\"models/prollama\",\n",
    "                  local_dir_use_symlinks=False,\n",
    "                  revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.77s/it]\n"
     ]
    }
   ],
   "source": [
    "## Alternative method: Pull using transformers\n",
    "\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# model_name = \"GreatCaptainNemo/ProLLaMA\"\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "# model.save_pretrained(\"/workspace/models/prollama\")\n",
    "# tokenizer.save_pretrained(\"/workspace/models/prollama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 30077, done.\u001b[K\n",
      "remote: Counting objects: 100% (122/122), done.\u001b[K\n",
      "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
      "remote: Total 30077 (delta 58), reused 92 (delta 46), pack-reused 29955\u001b[K\n",
      "Receiving objects: 100% (30077/30077), 54.56 MiB | 39.15 MiB/s, done.\n",
      "Resolving deltas: 100% (21407/21407), done.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.25.0 Requires-Python >=3.9; 1.25.0rc1 Requires-Python >=3.9; 1.25.1 Requires-Python >=3.9; 1.25.2 Requires-Python >=3.9; 1.26.0 Requires-Python <3.13,>=3.9; 1.26.0b1 Requires-Python <3.13,>=3.9; 1.26.0rc1 Requires-Python <3.13,>=3.9; 1.26.1 Requires-Python <3.13,>=3.9; 1.26.2 Requires-Python >=3.9; 1.26.3 Requires-Python >=3.9; 1.26.4 Requires-Python >=3.9; 2.0.0 Requires-Python >=3.9; 2.0.0b1 Requires-Python >=3.9; 2.0.0rc1 Requires-Python >=3.9; 2.0.0rc2 Requires-Python >=3.9; 2.0.1 Requires-Python >=3.9\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy~=1.26.4 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0rc1, 1.23.0rc2, 1.23.0rc3, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0rc1, 1.24.0rc2, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy~=1.26.4\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git && \\\n",
    "    pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the model to GGUF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: prollama\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F32, shape = {4096, 32000}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F32, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F32, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F32, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output.weight,               torch.float16 --> F32, shape = {4096, 32000}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 4096\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 11008\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 0\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 1\n",
      "INFO:gguf.vocab:Setting special token type eos to 2\n",
      "INFO:gguf.vocab:Setting special token type pad to 0\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:models/prollama/prollama.gguf: n_tensors = 291, total_size = 27.0G\n",
      "Writing: 100%|███████████████████████████| 27.0G/27.0G [02:05<00:00, 214Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to models/prollama/prollama.gguf\n"
     ]
    }
   ],
   "source": [
    "!python3 llama.cpp/convert_hf_to_gguf.py models/prollama/ \\\n",
    "  --outfile models/prollama/prollama.gguf \\\n",
    "  --outtype f32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
